{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Reinforcement Learning\n",
    "![rl loop](refs/rl_loop.png)\n",
    "> An RL algorithm seeks to maximize the agent’s total reward, given a previously unknown environment, through a trial-and-error learning process.\n",
    "\n",
    "Our solution will be a **policy**.\n",
    "![policy](refs/policy.png)\n",
    "\n",
    "## Deep Reinforcement Learning\n",
    "Solve RL problems through deep learning approach.\n",
    "\n",
    "## Methods\n",
    "![methods](refs/methods_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment (OpenAI Gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Pong-v0').unwrapped\n",
    "observation = env.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    # Record reward for future training\n",
    "    policy.rewards.append(reward)\n",
    "    reward_sum += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input (preprocessing)\n",
    "\n",
    "![before](refs/before_img.png)\n",
    "\n",
    "![after](refs/after_img.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    \"\"\" Preprocess 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = img[35:195]     # crop\n",
    "    I = I[::2, ::2, 0]  # downsample by factor of 2\n",
    "    I[I == 144] = 0     # erase background (background type 1)\n",
    "    I[I == 109] = 0     # erase background (background type 2)\n",
    "    I[I != 0] = 1       # everything else (paddles, ball) just set to 1\n",
    "\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyGradient(nn.Module):\n",
    "    \"\"\"\n",
    "    It's out model class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim):\n",
    "        super(PolicyGradient, self).__init__()\n",
    "        self.hidden = nn.Linear(in_dim, 200)\n",
    "        self.out = nn.Linear(200, 3)\n",
    "\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "        # Weights initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # 'n' is number of inputs to each neuron\n",
    "                n = len(m.weight.data[1])\n",
    "                # \"Xavier\" initialization\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.hidden(x))\n",
    "        logits = self.out(h)\n",
    "        return F.softmax(logits)\n",
    "\n",
    "    def reset(self):\n",
    "        del self.rewards[:]\n",
    "        del self.actions[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic policy\n",
    "\n",
    "We use stochastic policy which means our model produces probability distribution over all actions, _π(a | s) = probability of action given state_. Then we sample from this distribution in order to get action.\n",
    "\n",
    "Why stochastic policy?:\n",
    "\n",
    "* We can use the score function gradient estimator, which tries to make good actions more probable.\n",
    "* Stochastic environments.\n",
    "* Partially observable states.\n",
    "* The randomness inherent in the policy leads to exploration, which is crucial for most learning problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action(policy, observation):\n",
    "    # Get current state, which is difference between current and previous state\n",
    "    cur_state = preprocess(observation)\n",
    "    state = cur_state - get_action.prev_state \\\n",
    "        if get_action.prev_state is not None else np.zeros(len(cur_state))\n",
    "    get_action.prev_state = cur_state\n",
    "\n",
    "    var_state = Variable(\n",
    "        # Make torch FloatTensor from numpy array and add batch dimension\n",
    "        torch.from_numpy(state).type(FloatTensor).unsqueeze(0)\n",
    "    )\n",
    "    probabilities = policy(var_state)\n",
    "    # Stochastic policy: roll a biased dice to get an action\n",
    "    action = probabilities.multinomial()\n",
    "    # Record action for future training\n",
    "    policy.actions.append(action)\n",
    "    # '+ 1' converts action to valid Pong env action\n",
    "    return action.data[0, 0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "    \n",
    "Maximize log likelihood of true label (e.g. cross-entropy error).\n",
    "\n",
    "![sl](refs/sl_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Maximize log likelihood of good action and minimize it for bad actions (via advantage or on diagram \"eventual reward\").\n",
    "\n",
    "![sl](refs/rl_figure.png)\n",
    "\n",
    "> Policy Gradients: Run a policy for a while. See what actions led to high rewards. Increase their probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's play the game ;)\n",
    "while True:\n",
    "    [...]\n",
    "    \n",
    "    ### Here actions are taken in environment ###\n",
    "    action = get_action(policy, observation)\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    # Record reward for future training\n",
    "    policy.rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "\n",
    "    ### Here is our reinforcement learning logic ###\n",
    "    if done:\n",
    "        num_episodes += 1\n",
    "\n",
    "       [...]\n",
    "\n",
    "        # Reinforce actions\n",
    "        for action, reward in zip(policy.actions, rewards):\n",
    "            action.reinforce(reward)\n",
    "\n",
    "        # BACKPROP!!! (Gradients are accumulated each episode until update)\n",
    "        autograd.backward(policy.actions, [None for a in policy.actions])\n",
    "\n",
    "        ### Here we do weight update each batch ###\n",
    "        if num_episodes % HPARAMS.batch_size == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print \"### Updated parameters! ###\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discounted reward\n",
    "\n",
    "> In a more general RL setting we would receive some reward \\\\(r_t\\\\) at every time step. One common choice is to use a discounted reward, so the “eventual reward” in the diagram above would become:\n",
    "\n",
    "$$\n",
    "R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\\\\n",
    "0 \\leq \\gamma < 1\n",
    "$$\n",
    "\n",
    "But why discounted?\n",
    "* We care more about tomorrow than what will be sometime in the distant future.\n",
    "* In infinite horizont without discount we would get infinite rewards _(infinite in this case means troubles)_.\n",
    "\n",
    "### Know your limit!...\n",
    "\n",
    "$$\n",
    "0 \\leq \\gamma < 1 \\\\\n",
    "R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\leq \\sum_{k=0}^{\\infty} \\gamma^k R_{max} = \\frac{R_{max}}{1 - \\gamma}\n",
    "$$\n",
    "\n",
    "Infinite horizont has finite sum of discounted rewards...\n",
    "\n",
    "Why is this important?\n",
    "\n",
    "## Maximum Expected Utility (MEU) principle says...\n",
    "\n",
    "A rational agent should chose the action that maximizes its expected utility given its knowlage.\n",
    "\n",
    "###  Comparing policies\n",
    "\n",
    "Expected utility in state s with respect to policy:\n",
    "\n",
    "$$\n",
    "U^{\\pi}(s) = E[\\sum_{t = 0}^{\\infty}\\gamma^tR(S_t)] \\\\\n",
    "S_t - \\text{random variable representing state reached at time } t \\text{ following policy }  \\pi\n",
    "$$\n",
    "\n",
    "...where the expectation is with respect to the probability distribution over state sequences determined by s and π.\n",
    "\n",
    "Comparing infinities could be problematic..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute discounted reward\n",
    "discounted_R = []\n",
    "running_add = 0\n",
    "for reward in policy.rewards[::-1]:\n",
    "    if reward != 0:\n",
    "        # Reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = 0\n",
    "\n",
    "        running_add = running_add * HPARAMS.gamma + reward\n",
    "        # \"Further\" actions have less discounted rewards\n",
    "        discounted_R.insert(0, running_add)\n",
    "\n",
    "    rewards = FloatTensor(discounted_R)\n",
    "    # Standardize rewards\n",
    "    rewards = (rewards - rewards.mean()) / \\\n",
    "        (rewards.std() + np.finfo(np.float32).eps)\n",
    "    # Batch size shouldn't influence update step\n",
    "    rewards = rewards / HPARAMS.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving Policy Gradients\n",
    "\n",
    "## Score function gradient estimator\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} E_x[f(x)] &= \\nabla_{\\theta} \\sum_x p(x) f(x) & \\text{definition of expectation} \\\\\n",
    "& = \\sum_x \\nabla_{\\theta} p(x) f(x) & \\text{swap sum and gradient} \\\\\n",
    "& = \\sum_x p(x) \\frac{\\nabla_{\\theta} p(x)}{p(x)} f(x) & \\text{both multiply and divide by } p(x) \\\\\n",
    "& = \\sum_x p(x) \\nabla_{\\theta} \\log p(x) f(x) & \\text{use the fact that } \\nabla_{\\theta} \\log(z) = \\frac{1}{z} \\nabla_{\\theta} z \\\\\n",
    "& = E_x[f(x) \\nabla_{\\theta} \\log p(x) ] & \\text{definition of expectation}\n",
    "\\end{align}\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "![policy distribution](refs/pg_distribution_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights visualization\n",
    "\n",
    "![weights](refs/weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "* Hiperparameters tuning\n",
    "* ConvNets\n",
    "* Move penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_References:_\n",
    "* Andrej Karpathy's blog post, [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)\n",
    "* John Schulmann's thesis, [Optimizing Expectations: From Deep Reinforcement Learning to Stochastic Computation Graphs](http://joschu.net/docs/thesis.pdf)\n",
    "* Dan Klein and Pieter Abbeel, [CS188 Berkeley course](http://ai.berkeley.edu/home.html)\n",
    "* Stuart Russell and Peter Norvig, _Artificial Intelligence: A Modern Approach, 3rd Edition_\n",
    "* Richard S. Sutton and Andrew G. Barto, _Reinforcement Learning: An Introduction Second edition, in progress \\*\\*\\*\\*Draft\\*\\*\\*\\*_\n",
    "\n",
    "_Also:_\n",
    "* https://www.quora.com/How-do-we-benefit-from-stochastic-policies-in-reinforcement-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
