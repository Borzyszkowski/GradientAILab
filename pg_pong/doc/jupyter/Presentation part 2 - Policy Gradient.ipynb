{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Policy Gradients using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "1. What reinforcement learning is all about?\n",
    "   1. Introduction\n",
    "   2. Deep Reinforcement Learning\n",
    "   3. Methods to solve reinforcement learning problems\n",
    "   4. Insight into Policy Gradients\n",
    "2. PG Pong\n",
    "   1. Environment (OpenAI Gym)\n",
    "   2. Input (preprocessing)\n",
    "   3. Model (policy network)\n",
    "   4. How to decide what action to take and why with stochastic policy?\n",
    "   5. Learning!\n",
    "3. Extras\n",
    "   1. Why to discount rewards?\n",
    "   2. Deriving Policy Gradients from score function gradient estimator\n",
    "   3. Policy distribution shifting interpretation\n",
    "   4. Weights visualization\n",
    "   5. Improvement ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What reinforcement learning is all about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Reinforcement Learning is a framework to formalize substantial amount of reward-related learning problems\n",
    "![rl loop](refs/rl_loop.png)\n",
    "> An RL algorithm seeks to maximize the agent’s expected return (total future reward), given a previously unknown environment, through a trial-and-error learning process.\n",
    "\n",
    "Our solution will be a **policy**.\n",
    "![policy](refs/policy.png)\n",
    "\n",
    "## Deep Reinforcement Learning\n",
    "Solve RL problems through deep learning.\n",
    "\n",
    "## Methods\n",
    "![methods](refs/methods_overview.png)\n",
    "\n",
    "All of them seek to maximize expected return but in different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "\n",
    "## Expected return:\n",
    "$$\n",
    "J\\left(  \\mathbf{\\theta}\\right)  =\\mathbf{E}\\left[  \\sum\\nolimits_{t=0}^{T}R_{t}\\right] \\\\\n",
    "R_t - \\text{random variable representing reward reached at time } t \\\\\n",
    "\\text{ following policy } \\pi \\text{ from some initial state} \\\\\n",
    "T - \\text{final time step or end of the episode}\n",
    "$$\n",
    "\n",
    "All we do is finding gradient estimate of expected return to do stochastic gradient ascend update!\n",
    "\n",
    "## Convergence\n",
    "> If the gradient estimate is unbiased and learning rates fulfill \\\\(\\sum\\textstyle_{h=0}^{\\infty}\\alpha_{h}>0\\\\) and \\\\(\\sum\\textstyle_{h=0}^{\\infty}\\alpha_{h}^{2}=\\textrm{const}\\ ,\\\\) the learning process is guaranteed to converge at least to a local minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PG Pong\n",
    "# Environment (OpenAI Gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Pong-v0').unwrapped\n",
    "observation = env.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    # Record reward for future training\n",
    "    policy.rewards.append(reward)\n",
    "    reward_sum += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input (preprocessing)\n",
    "\n",
    "![before](refs/before_img.png)\n",
    "\n",
    "![after](refs/after_img.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    \"\"\" Preprocess 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = img[35:195]     # crop\n",
    "    I = I[::2, ::2, 0]  # downsample by factor of 2\n",
    "    I[I == 144] = 0     # erase background (background type 1)\n",
    "    I[I == 109] = 0     # erase background (background type 2)\n",
    "    I[I != 0] = 1       # everything else (paddles, ball) just set to 1\n",
    "\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyGradient(nn.Module):\n",
    "    \"\"\"\n",
    "    It's out model class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim):\n",
    "        super(PolicyGradient, self).__init__()\n",
    "        self.hidden = nn.Linear(in_dim, 200)\n",
    "        self.out = nn.Linear(200, 3)\n",
    "\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "        # Weights initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # 'n' is number of inputs to each neuron\n",
    "                n = len(m.weight.data[1])\n",
    "                # \"Xavier\" initialization\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.hidden(x))\n",
    "        logits = self.out(h)\n",
    "        return F.softmax(logits)\n",
    "\n",
    "    def reset(self):\n",
    "        del self.rewards[:]\n",
    "        del self.actions[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic policy\n",
    "\n",
    "We use stochastic policy which means our model produces probability distribution over all actions, _π(a | s) = probability of action given state_. Then we sample from this distribution in order to get action.\n",
    "\n",
    "Why stochastic policy?:\n",
    "\n",
    "* We can use the score function gradient estimator, which tries to make good actions more probable.\n",
    "* Stochastic environments.\n",
    "* Partially observable states.\n",
    "* The randomness inherent in the policy leads to exploration, which is crucial for most learning problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action(policy, observation):\n",
    "    # Get current state, which is difference between current and previous state\n",
    "    cur_state = preprocess(observation)\n",
    "    state = cur_state - get_action.prev_state \\\n",
    "        if get_action.prev_state is not None else np.zeros(len(cur_state))\n",
    "    get_action.prev_state = cur_state\n",
    "\n",
    "    var_state = Variable(\n",
    "        # Make torch FloatTensor from numpy array and add batch dimension\n",
    "        torch.from_numpy(state).type(FloatTensor).unsqueeze(0)\n",
    "    )\n",
    "    probabilities = policy(var_state)\n",
    "    # Stochastic policy: roll a biased dice to get an action\n",
    "    action = probabilities.multinomial()\n",
    "    # Record action for future training\n",
    "    policy.actions.append(action)\n",
    "    # '+ 1' converts action to valid Pong env action\n",
    "    return action.data[0, 0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "    \n",
    "Maximize log likelihood of true label (e.g. cross-entropy error).\n",
    "\n",
    "![sl](refs/sl_figure.png)\n",
    "\n",
    "### **Loss:** \\\\( \\sum_i log p(\\text{a } \\vert \\text{ img}) \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Maximize log likelihood of good action and minimize it for bad actions (via advantage or on diagram \"eventual reward\").\n",
    "\n",
    "![sl](refs/rl_figure.png)\n",
    "\n",
    "> Policy Gradients: Run a policy for a while. See what actions led to high rewards. Increase their probability.\n",
    "\n",
    "### **Loss:** \\\\( \\sum_i A_i log p(\\text{a } \\vert \\text{ img}) \\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's play the game ;)\n",
    "while True:\n",
    "    [...]\n",
    "    \n",
    "    ### Here actions are taken in environment ###\n",
    "    action = get_action(policy, observation)\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    # Record reward for future training\n",
    "    policy.rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "\n",
    "    ### Here is our reinforcement learning logic ###\n",
    "    if done:\n",
    "        num_episodes += 1\n",
    "\n",
    "       [...]\n",
    "\n",
    "        # Reinforce actions\n",
    "        for action, reward in zip(policy.actions, rewards):\n",
    "            action.reinforce(reward)\n",
    "\n",
    "        # BACKPROP!!! (Gradients are accumulated each episode until update)\n",
    "        autograd.backward(policy.actions, [None for a in policy.actions])\n",
    "\n",
    "        ### Here we do weight update each batch ###\n",
    "        if num_episodes % HPARAMS.batch_size == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print \"### Updated parameters! ###\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discounted reward\n",
    "\n",
    "> In a more general RL setting we would receive some reward \\\\(r_t\\\\) at every time step. One common choice is to use a discounted reward, so the “eventual reward” in the diagram above would become:\n",
    "\n",
    "$$\n",
    "R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\\\\n",
    "0 \\leq \\gamma < 1\n",
    "$$\n",
    "\n",
    "But why discounted?\n",
    "* We care more about tomorrow than what will be sometime in the distant future.\n",
    "* In infinite horizont without discount we would get infinite rewards _(infinite in this case means troubles)_.\n",
    "\n",
    "### Know your limit!...\n",
    "\n",
    "$$\n",
    "0 \\leq \\gamma < 1 \\\\\n",
    "R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\leq \\sum_{k=0}^{\\infty} \\gamma^k R_{max} = \\frac{R_{max}}{1 - \\gamma}\n",
    "$$\n",
    "\n",
    "Infinite horizont has finite sum of discounted rewards...\n",
    "\n",
    "Why is this important?\n",
    "\n",
    "## Maximum Expected Utility (MEU) principle says...\n",
    "\n",
    "A rational agent should chose the action that maximizes its expected utility given its knowlage.\n",
    "\n",
    "Expected utility in state s with respect to policy:\n",
    "\n",
    "$$\n",
    "U^{\\pi}(s) = E[\\sum_{t = 0}^{\\infty}\\gamma^tR(S_t)] \\\\\n",
    "S_t - \\text{random variable representing state reached at time } t \\text{ following policy }  \\pi\n",
    "$$\n",
    "\n",
    "...where the expectation is with respect to the probability distribution over state sequences determined by s and π.\n",
    "\n",
    "**Comparing infinities could be problematic...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute discounted reward\n",
    "discounted_R = []\n",
    "running_add = 0\n",
    "for reward in policy.rewards[::-1]:\n",
    "    if reward != 0:\n",
    "        # Reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = 0\n",
    "\n",
    "        running_add = running_add * HPARAMS.gamma + reward\n",
    "        # \"Further\" actions have less discounted rewards\n",
    "        discounted_R.insert(0, running_add)\n",
    "\n",
    "    rewards = FloatTensor(discounted_R)\n",
    "    # Standardize rewards\n",
    "    rewards = (rewards - rewards.mean()) / \\\n",
    "        (rewards.std() + np.finfo(np.float32).eps)\n",
    "    # Batch size shouldn't influence update step\n",
    "    rewards = rewards / HPARAMS.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving Policy Gradients\n",
    "\n",
    "## Score function gradient estimator\n",
    "\n",
    "$$\n",
    "f(x) - \\text{scalar valued score function (our reward function).} \\\\\n",
    "p(x|\\theta) - \\text{probability distribution parametrized by } \\theta \\text{ (our policy network).}\n",
    "$$\n",
    "\n",
    "> We are interested in finding how we should shift the distribution (through its parameters θ) to increase the scores of its samples, as judged by f (i.e. how do we change the network’s parameters so that action samples get higher rewards).\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} E_{x \\sim p(x | \\theta)}[f(x)] &= \\nabla_{\\theta} \\sum_x p(x) f(x) & \\text{definition of expectation} \\\\\n",
    "& = \\sum_x \\nabla_{\\theta} p(x) f(x) & \\text{swap sum and gradient} \\\\\n",
    "& = \\sum_x p(x) \\frac{\\nabla_{\\theta} p(x)}{p(x)} f(x) & \\text{both multiply and divide by } p(x) \\\\\n",
    "& = \\sum_x p(x) \\nabla_{\\theta} \\log p(x) f(x) & \\text{use the fact that } \\nabla_{\\theta} \\log(z) = \\frac{1}{z} \\nabla_{\\theta} z \\\\\n",
    "& = E_x[f(x) \\nabla_{\\theta} \\log p(x) ] & \\text{definition of expectation} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) \\nabla_{\\theta} \\log p(x) - \\text{unbiased gradient estimator!}\n",
    "$$\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "![policy distribution](refs/pg_distribution_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights visualization\n",
    "\n",
    "![weights](refs/weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "* Hiperparameters tuning\n",
    "* ConvNets\n",
    "* Move penalty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_References:_\n",
    "* Andrej Karpathy's blog post, [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)\n",
    "* John Schulmann's thesis, [Optimizing Expectations: From Deep Reinforcement Learning to Stochastic Computation Graphs](http://joschu.net/docs/thesis.pdf)\n",
    "* Dan Klein and Pieter Abbeel, [CS188 Berkeley course](http://ai.berkeley.edu/home.html)\n",
    "* Stuart Russell and Peter Norvig, _Artificial Intelligence: A Modern Approach, 3rd Edition_\n",
    "* Richard S. Sutton and Andrew G. Barto, _Reinforcement Learning: An Introduction Second edition, in progress \\*\\*\\*\\*Draft\\*\\*\\*\\*_\n",
    "\n",
    "_Also:_\n",
    "* http://www.scholarpedia.org/article/Policy_gradient_methods\n",
    "* https://www.quora.com/How-do-we-benefit-from-stochastic-policies-in-reinforcement-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
